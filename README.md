#### üöÄ Databricks Lakehouse Pipeline using PySpark, Delta Lake & dbt

#### üìå Project Overview
This project implements an end-to-end **Databricks Lakehouse data engineering pipeline** using **PySpark Structured Streaming**, **Delta Lake**, and **dbt**.  
The pipeline ingests multiple CSV files from **Databricks Volumes**, loads them into a **Bronze layer**, applies reusable transformation logic, and incrementally upserts data into a **Silver layer** using **Delta MERGE**.  
The curated datasets are further modeled using **dbt models and snapshots (Slowly Changing Dimensions ‚Äì SCD)**.

The solution is designed to be **dynamic, reusable, and scalable** across multiple business entities such as **customers, trips, vehicles, payments, drivers, and locations**.

#### üèóÔ∏è Architecture

CSV files (Databricks Volumes)
‚Üì
PySpark Structured Streaming (trigger once)
‚Üì
Bronze Delta Tables
‚Üì
Reusable PySpark Transformations
‚Üì
Delta MERGE (CDC-based upsert)
‚Üì
Silver Delta Tables
‚Üì
dbt Models & Snapshots (SCD)

#### üß© Technologies Used

- Databricks
- PySpark
- Structured Streaming
- Delta Lake
- dbt
- GitHub (Databricks Repos)

#### üìÇ Project Structure

Pyspark_DBT_Deployment/
‚îú‚îÄ‚îÄ notebooks/
‚îÇ ‚îî‚îÄ‚îÄ ingestion and transformation notebooks
‚îú‚îÄ‚îÄ utils/
‚îÇ ‚îî‚îÄ‚îÄ customutils.py
‚îú‚îÄ‚îÄ dbt/
‚îÇ ‚îú‚îÄ‚îÄ dbt_project.yml
‚îÇ ‚îú‚îÄ‚îÄ models/
‚îÇ ‚îú‚îÄ‚îÄ snapshots/
‚îÇ ‚îî‚îÄ‚îÄ macros/
‚îî‚îÄ‚îÄ README.md


#### üîÅ End-to-End Workflow

#### 1Ô∏è‚É£ Source Ingestion ‚Äì Bronze Layer

Source CSV files are ingested from Databricks Volumes using Structured Streaming.

```python
spark.readStream.format("csv") \
  .option("header","true") \
  .schema(schema) \
  .load("/Volumes/.../source_data/customers/")
Multiple entities are processed dynamically:

entities = ["customers","trips","vehicles","payments","drivers","locations"]

######Streaming Write to Bronze Tables

.writeStream \
  .format("delta") \
  .trigger(once=True) \
  .option("checkpointLocation", "...") \
  .toTable("pyspark_dbt.bronze.customers")
This approach enables incremental ingestion using micro-batch streaming.

#### 3Ô∏è‚É£ Reusable Transformation Framework

Python utility class is created to standardize transformations across all entities.

#### 3.1 Deduplication using CDC logic

```python
row_number().over(
    Window.partitionBy("dedupKey").orderBy(desc(cdc))
)
This keeps only the latest record per business key based on the CDC timestamp.

##### 3.2 Audit column generation
This column tracks when a record is processed by the pipeline.

### Incremental Upsert into Silver Layer

ensures:

--existing records are updated

--new records are inserted

--older data does not overwrite newer data

####Dynamic Multi-Entity Processing
for entity in entities:
    obj.upsert(...)


The same pipeline logic is reused for all datasets.

6Ô∏è‚É£ dbt Modeling Layer

Example dbt model:

select *
from {{ source("source_bronze", "trips") }}


####Jinja templating is used to dynamically generate SQL.

7Ô∏è‚É£ Slowly Changing Dimensions using dbt Snapshots

dbt snapshots are used to track historical changes for dimension tables such as DimVehicle.

8Ô∏è‚É£ Version Control using GitHub

All notebooks, PySpark utilities, and dbt projects are maintained in a Databricks Repo and pushed to GitHub.
Only code is versioned (data and Delta tables are excluded).

üìò Concepts Implemented and Learned
‚úî PySpark Structured Streaming
.trigger(once=True)


#### Used streaming as an incremental ingestion mechanism.

‚úî Delta Lake CDC-based MERGE
.whenMatchedUpdateAll(condition="src.ts >= trg.ts")
.whenNotMatchedInsertAll()

‚úî Window Functions for Deduplication
row_number().over(Window.partitionBy(...).orderBy(...))

‚úî Reusable Transformation Utilities

Transformation logic is encapsulated inside a reusable Python class.

‚úî Dynamic and Scalable Pipelines
for entity in entities:

‚úî Schema and Merge Debugging

Handled:

merge key mismatches

data type mismatches

duplicate inserts during upserts

incorrect CDC logic

‚úî dbt Jinja Templating
{% for col in cols %}
  {{ col }}
{% endfor %}

‚úî dbt Sources and Snapshots
{{ source("source_bronze","trips") }}


and SCD snapshots for historical tracking.

‚úî Databricks Platform Features

Unity Catalog tables

Databricks Volumes

Databricks Repos

Streaming checkpoints

‚úî GitHub-based Version Control

structured repository layout

commit and push using Databricks Repos

.gitignore best practices

üèÅ Summary

This project demonstrates a production-oriented data engineering solution built on the Databricks Lakehouse platform.
It combines incremental ingestion, CDC-driven upserts, reusable PySpark transformations, scalable multi-entity processing,
and dbt-based analytical modeling with SCD tracking, all managed through GitHub for version control.



